{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933692a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x2540cf47a80>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                DummyTransformerBlock(cfg)\n",
    "                for _ in range(cfg[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x+0.044715*torch.pow(x,3))))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label), in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1,2,i)\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.layers(x)\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "  def __init__(self, layer_sizes, use_shortcut):\n",
    "    super().__init__()\n",
    "    self.use_shortcut = use_shortcut\n",
    "    self.layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU()),\n",
    "    ])\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      layer_output = layer(x)\n",
    "      if self.use_shortcut and x.shape == layer_output.shape:\n",
    "        x = x + layer_output\n",
    "      else:\n",
    "        x = layer_output\n",
    "    return x\n",
    "\n",
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "\n",
    "def print_gradients(model, x):\n",
    "  output = model(x)\n",
    "  target = torch.tensor([[0.]])\n",
    "\n",
    "  loss = nn.MSELoss()\n",
    "  loss = loss(output, target)\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "print_gradients(model_without_shortcut,sample_input)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut,sample_input)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads==0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out //num_heads\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b,num_tokens,self.num_heads,self.head_dim\n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b,num_tokens,self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"]\n",
    "    )\n",
    "    self.ff=FeedForward(cfg)\n",
    "    self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.att(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "    return x\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shapel:\", output.shape)\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "    )\n",
    "\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "    )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "    pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "    )\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    ")\n",
    "print(\n",
    "    f\"Number of trainable parameters\"\n",
    "    f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")\n",
    "\n",
    "def count_module_parameters(module):\n",
    "    \"\"\"计算指定模块的参数数量\"\"\"\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def analyze_model_parameters(model):\n",
    "    \"\"\"分析模型中各个模块的参数数量\"\"\"\n",
    "\n",
    "    print(\"=== 模型参数分析 ===\\n\")\n",
    "\n",
    "    # 1. 计算总参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"总参数量: {total_params:,}\")\n",
    "\n",
    "    # 2. 分析各个模块的参数量\n",
    "    print(\"\\n--- 各模块参数量 ---\")\n",
    "\n",
    "    # Token Embedding\n",
    "    tok_emb_params = count_module_parameters(model.tok_emb)\n",
    "    print(f\"Token Embedding: {tok_emb_params:,}\")\n",
    "\n",
    "    # Positional Embedding\n",
    "    pos_emb_params = count_module_parameters(model.pos_emb)\n",
    "    print(f\"Positional Embedding: {pos_emb_params:,}\")\n",
    "\n",
    "    # Transformer Blocks\n",
    "    trf_blocks_params = count_module_parameters(model.trf_blocks)\n",
    "    print(f\"Transformer Blocks: {trf_blocks_params:,}\")\n",
    "\n",
    "    # Final Layer Norm\n",
    "    final_norm_params = count_module_parameters(model.final_norm)\n",
    "    print(f\"Final Layer Norm: {final_norm_params:,}\")\n",
    "\n",
    "    # Output Head\n",
    "    out_head_params = count_module_parameters(model.out_head)\n",
    "    print(f\"Output Head: {out_head_params:,}\")\n",
    "\n",
    "    # 3. 深入分析Transformer Blocks\n",
    "    print(\"\\n--- Transformer Blocks 详细分析 ---\")\n",
    "\n",
    "    attention_params_total = 0\n",
    "    feedforward_params_total = 0\n",
    "    norm_params_total = 0\n",
    "\n",
    "    for i, block in enumerate(model.trf_blocks):\n",
    "        # 注意力模块参数\n",
    "        att_params = count_module_parameters(block.att)\n",
    "        attention_params_total += att_params\n",
    "\n",
    "        # 前馈模块参数\n",
    "        ff_params = count_module_parameters(block.ff)\n",
    "        feedforward_params_total += ff_params\n",
    "\n",
    "        # 层归一化参数\n",
    "        norm1_params = count_module_parameters(block.norm1)\n",
    "        norm2_params = count_module_parameters(block.norm2)\n",
    "        norm_params_total += norm1_params + norm2_params\n",
    "\n",
    "        print(f\"Block {i+1}:\")\n",
    "        print(f\"  - 注意力模块: {att_params:,}\")\n",
    "        print(f\"  - 前馈模块: {ff_params:,}\")\n",
    "        print(f\"  - 层归一化: {norm1_params + norm2_params:,}\")\n",
    "\n",
    "    # 4. 对比分析\n",
    "    print(\"\\n--- 参数对比分析 ---\")\n",
    "    print(f\"注意力模块总参数: {attention_params_total:,}\")\n",
    "    print(f\"前馈模块总参数: {feedforward_params_total:,}\")\n",
    "    print(f\"层归一化总参数: {norm_params_total:,}\")\n",
    "\n",
    "    # 计算比例\n",
    "    attention_ratio = attention_params_total / total_params * 100\n",
    "    feedforward_ratio = feedforward_params_total / total_params * 100\n",
    "    norm_ratio = norm_params_total / total_params * 100\n",
    "\n",
    "    print(f\"\\n参数占比:\")\n",
    "    print(f\"注意力模块: {attention_ratio:.1f}%\")\n",
    "    print(f\"前馈模块: {feedforward_ratio:.1f}%\")\n",
    "    print(f\"层归一化: {norm_ratio:.1f}%\")\n",
    "\n",
    "    # 5. 理论计算验证\n",
    "    print(\"\\n--- 理论计算验证 ---\")\n",
    "    emb_dim = 768\n",
    "    n_heads = 12\n",
    "    n_layers = 12\n",
    "\n",
    "    # 注意力模块理论参数\n",
    "    # 每个注意力头: 3 * (768 * 64) + (768 * 768) = 147,456 + 589,824 = 737,280\n",
    "    # 12层: 737,280 * 12 = 8,847,360\n",
    "    theoretical_att = n_layers * (3 * emb_dim * (emb_dim // n_heads) + emb_dim * emb_dim)\n",
    "    print(f\"注意力模块理论参数: {theoretical_att:,}\")\n",
    "\n",
    "    # 前馈模块理论参数\n",
    "    # 每层: 768 * 3072 + 3072 * 768 = 2,359,296 + 2,359,296 = 4,718,592\n",
    "    # 12层: 4,718,592 * 12 = 56,623,104\n",
    "    theoretical_ff = n_layers * (emb_dim * (4 * emb_dim) + (4 * emb_dim) * emb_dim)\n",
    "    print(f\"前馈模块理论参数: {theoretical_ff:,}\")\n",
    "\n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'attention': attention_params_total,\n",
    "        'feedforward': feedforward_params_total,\n",
    "        'norm': norm_params_total\n",
    "    }\n",
    "\n",
    "# 运行分析\n",
    "print(\"开始分析模型参数...\")\n",
    "params_analysis = analyze_model_parameters(model)\n",
    "\n",
    "# 可视化对比\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建饼图\n",
    "labels = ['Attention', 'Forward', 'LayerNorm', 'Other']\n",
    "sizes = [\n",
    "    params_analysis['attention'],\n",
    "    params_analysis['feedforward'],\n",
    "    params_analysis['norm'],\n",
    "    params_analysis['total'] - params_analysis['attention'] - params_analysis['feedforward'] - params_analysis['norm']\n",
    "]\n",
    "\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('parameters distribution')\n",
    "\n",
    "# 创建柱状图\n",
    "plt.subplot(1, 2, 2)\n",
    "modules = ['Token Emb', 'Pos Emb', 'Attention', 'FeedForward', 'Layer Norm', 'Output Head']\n",
    "param_counts = [\n",
    "    count_module_parameters(model.tok_emb),\n",
    "    count_module_parameters(model.pos_emb),\n",
    "    params_analysis['attention'],\n",
    "    params_analysis['feedforward'],\n",
    "    params_analysis['norm'],\n",
    "    count_module_parameters(model.out_head)\n",
    "]\n",
    "\n",
    "plt.bar(modules, param_counts, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ffb366', '#c2c2f0'])\n",
    "plt.title('Blocks')\n",
    "plt.ylabel('parameters')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024*1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "  for _ in range(max_new_tokens):\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "    with torch.no_grad():\n",
    "      logits = model(idx_cond)\n",
    "\n",
    "    logits = logits[:, -1, :]\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "  return idx\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\",encoded_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\",out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n",
    "\n",
    "import torch\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "import tiktoken\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "  return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "  flat = token_ids.squeeze(0)\n",
    "  return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [16833,3626,6100],\n",
    "    [40,1107,588]\n",
    "])\n",
    "targets = torch.tensor([\n",
    "    [3626,6100,345],\n",
    "    [1107,588,11311]\n",
    "])\n",
    "with torch.no_grad():\n",
    "  logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "   f\" {token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\",targets_flat.shape)\n",
    "\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "perplexity\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "  text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader=DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x,y in train_loader:\n",
    "  print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x,y in val_loader:\n",
    "  print(x.shape, y.shape)\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "  input_batch = input_batch.to(device)\n",
    "  target_batch = target_batch.to(device)\n",
    "  logits = model(input_batch)\n",
    "  loss = torch.nn.functional.cross_entropy(\n",
    "      logits.flatten(0,1), target_batch.flatten()\n",
    "  )\n",
    "  return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "  total_loss = 0.\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = calc_loss_batch(\n",
    "          input_batch, target_batch, model, device\n",
    "      )\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "  return total_loss / num_batches\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                       num_epochs, eval_freq, eval_iter, start_context,tokenizer):\n",
    "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "  tokens_seen, global_step = 0, -1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = calc_loss_batch(\n",
    "          input_batch, target_batch, model, device\n",
    "      )\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tokens_seen += input_batch.numel()\n",
    "      global_step += 1\n",
    "\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model(\n",
    "            model, train_loader, val_loader, device, eval_iter\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(\n",
    "            f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "            f\"Train loss {train_loss:.3f},\"\n",
    "            f\"Val loss {val_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    generate_and_print_sample(\n",
    "        model, tokenizer, device, start_context\n",
    "    )\n",
    "  return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=eval_iter\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=eval_iter\n",
    "    )\n",
    "  model.train()\n",
    "  return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "  model.eval()\n",
    "  context_size = model.pos_emb.weight.shape[0]\n",
    "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "  with torch.no_grad():\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,idx=encoded,\n",
    "        max_new_tokens=50,context_size=context_size\n",
    "    )\n",
    "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "  print(decoded_text.replace(\"\\n\",\" \"))\n",
    "  model.train()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,weight_decay=0.1\n",
    ")\n",
    "num_epochs=10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,train_loader,val_loader,optimizer,device,\n",
    "    num_epochs=num_epochs,eval_freq=5,eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen,tokens_seen,train_losses,val_losses):\n",
    "  fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "  ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n",
    "  ax1.plot(\n",
    "      epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "  )\n",
    "  ax1.set_xlabel(\"Epochs\")\n",
    "  ax1.set_ylabel(\"Loss\")\n",
    "  ax1.legend(loc=\"upper right\")\n",
    "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax2 = ax1.twiny()\n",
    "  ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "  ax2.set_xlabel(\"Tokens seen\")\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89,-1.90,6.75,1.63,-1.62,-1.89,6.28,1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits,dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "def print_sample_token(probas):\n",
    "  torch.manual_seed(123)\n",
    "  sample = [torch.multinomial(probas,num_samples=1).item()\n",
    "        for i in range(1_000)]\n",
    "  sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "  for i, freq in enumerate(sampled_ids):\n",
    "    print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sample_token(probas)\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "  scaled_logits = logits / temperature\n",
    "  return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "         for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i,T in enumerate(temperatures):\n",
    "  rects = ax.bar(x + i*bar_width,scaled_probas[i],\n",
    "          bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(new_logits)\n",
    "\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)\n",
    "\n",
    "def generate(model,idx,max_new_tokens,context_size,\n",
    "             temperature=0.0,top_k=None,eos_id=None):\n",
    "  for _ in range(max_new_tokens):\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "    with torch.no_grad():\n",
    "      logits = model(idx_cond)\n",
    "    logits = logits[:, -1, :]\n",
    "    if top_k is not None:\n",
    "      top_logits, _ = torch.topk(logits, top_k)\n",
    "      min_val = top_logits[:, -1]\n",
    "      logits = torch.where(\n",
    "          logits < min_val,\n",
    "          torch.tensor(float('-inf')).to(logits.device),\n",
    "          logits\n",
    "      )\n",
    "    if temperature > 0.0:\n",
    "      logits = logits / temperature\n",
    "      probs = torch.softmax(logits,dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    else:\n",
    "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    if idx_next ==eos_id:\n",
    "      break\n",
    "    idx = torch.cat((idx,idx_next),dim=-1)\n",
    "  return idx\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\",map_location=device))\n",
    "model.eval()\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "  },\n",
    "    \"model_and_optimizer.pth\")\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "model.train();\n",
    "\n",
    "!pip show tensorflow tqdm\n",
    "\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url,filename)\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "settings,params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "print(\"Settings:\",settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n",
    "\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\":768, \"n_layers\":12, \"n_heads\":12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\":1024, \"n_layers\":24, \"n_heads\":16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\":1280, \"n_layers\":36, \"n_heads\":20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\":1600, \"n_layers\":48, \"n_heads\":25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "NEW_CONFIG.update({\"context_length\":1024})\n",
    "\n",
    "NEW_CONFIG.update({\"qkv_bias\":True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "def assign(left,right):\n",
    "  if left.shape != right.shape:\n",
    "    raise ValueError(f\"Shape mismatch. Left:{left.shape},\"\n",
    "              \"Right:{right.shape}\")\n",
    "  return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "  gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "  gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "  for b in range(len(params[\"blocks\"])):\n",
    "    q_w, k_w,v_w = np.split(\n",
    "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"],3,axis=-1\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "        gpt.trf_blocks[b].att.W_query.weight,q_w.T\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "        gpt.trf_blocks[b].att.W_key.weight,k_w.T\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "        gpt.trf_blocks[b].att.W_value.weight,v_w.T\n",
    "    )\n",
    "\n",
    "    q_b, k_b, v_b = np.split(\n",
    "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"],3,axis=-1\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "        gpt.trf_blocks[b].att.W_query.bias,q_b\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "        gpt.trf_blocks[b].att.W_key.bias,k_b\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "        gpt.trf_blocks[b].att.W_value.bias,v_b\n",
    "    )\n",
    "\n",
    "    gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "        gpt.trf_blocks[b].att.out_proj.weight,\n",
    "        params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "    )\n",
    "    gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "        gpt.trf_blocks[b].att.out_proj.bias,\n",
    "        params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "    )\n",
    "    gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "    )\n",
    "\n",
    "    gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "    )\n",
    "    gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "    )\n",
    "    gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "    )\n",
    "\n",
    "    gpt.trf_blocks[b].norm1.scale = assign(\n",
    "        gpt.trf_blocks[b].norm1.scale,\n",
    "        params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "    )\n",
    "    gpt.trf_blocks[b].norm1.shift = assign(\n",
    "        gpt.trf_blocks[b].norm1.shift,\n",
    "        params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "    )\n",
    "    gpt.trf_blocks[b].norm2.scale = assign(\n",
    "        gpt.trf_blocks[b].norm2.scale,\n",
    "        params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "    )\n",
    "    gpt.trf_blocks[b].norm2.shift = assign(\n",
    "        gpt.trf_blocks[b].norm2.shift,\n",
    "        params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "    )\n",
    "  \n",
    "  gpt.final_norm.scale = assign(gpt.final_norm.scale,params[\"g\"])\n",
    "  gpt.final_norm.shift = assign(gpt.final_norm.shift,params[\"b\"])\n",
    "  gpt.out_head.weight = assign(gpt.out_head.weight,params[\"wte\"])\n",
    "\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "def download_and_unzip_spam_data(url,zip_path,extracted_path,data_file_path):\n",
    "  if data_file_path.exists():\n",
    "    print(f\"{data_file_path} already exists. Skipping download\"\n",
    "        \"and extraction.\")\n",
    "    return\n",
    "\n",
    "  with urllib.request.urlopen(url) as response:\n",
    "    with open(zip_path, \"wb\") as out_file:\n",
    "      out_file.write(response.read())\n",
    "\n",
    "  with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extracted_path)\n",
    "\n",
    "  original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "  os.rename(original_file_path, data_file_path)\n",
    "  print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url,zip_path,extracted_path,data_file_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\",\"Text\"]\n",
    ")\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
